<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="./img/favicon.ico">

        <title>Periscope</title>
        <link href="./css/sequenceiq.doc.min.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>

            <!-- Main title -->
            <a class="navbar-brand" href=".">Periscope</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            <!-- Main navigation -->
            <ul class="nav navbar-nav">
            
            </ul>

            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                <li class="disabled">
                    <a rel="next" >
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li class="disabled">
                    <a rel="prev" >
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#periscope">Periscope</a></li>
        
            <li><a href="#overview">Overview</a></li>
        
            <li><a href="#how-it-works">How it works</a></li>
        
            <li><a href="#technology">Technology</a></li>
        
            <li><a href="#building-blocks">Building blocks</a></li>
        
            <li><a href="#quickstart-and-installation">QuickStart and installation</a></li>
        
            <li><a href="#monitoring">Monitoring</a></li>
        
            <li><a href="#releases-future-plans">Releases, future plans</a></li>
        
            <li><a href="#contribution">Contribution</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="periscope">Periscope</h1>
<p><em>Periscope is a powerful, fast, thick and top-to-bottom right-hander, eastward from Sumbawa's famous west-coast. Timing is critical, as needs a number of elements to align before it shows its true colors.</em></p>
<p><em>Periscope brings QoS and autoscaling to Hadoop YARN. Built on cloud resource management and YARN schedulers, allows to associate SLA policies to applications.</em></p>
<p>Periscope <a href="http://docs.periscope.apiary.io/">API documentation</a>.</p>
<h2 id="overview">Overview</h2>
<p>The purpose of Periscope is to bring QoS to a multi-tenant Hadoop cluster, while allowing to apply SLA policies to individual applications.
At <a href="http://sequenceiq.com">SequenceIQ</a> working with multi-tenant Hadoop clusters for quite a while we have always seen the same frustration and fight for resource between users.
The <strong>FairScheduler</strong> was partially solving this problem - bringing in fairness based on the notion of <a href="http://static.usenix.org/event/nsdi11/tech/full_papers/Ghodsi.pdf">Dominant Resource Fairness</a>.
With the emergence of Hadoop 2 YARN and the <strong>CapacityScheduler</strong> we had the option to maximize throughput and utilization for a multi-tenant cluster in an operator-friendly manner.
The scheduler works around the concept of queues. These queues are typically setup by administrators to reflect the economics of the shared cluster.
While this is a pretty good abstraction and brings some level of SLA for predictable workloads, it often needs proper design ahead.
The queue hierarchy and resource allocation needs to be changed when new tenants and workloads are moved to the cluster.</p>
<p>Periscope was designed around the idea of <code>autoscaling</code> clusters - without any need to preconfigure queues, cluster nodes or apply capacity planning ahead.</p>
<h2 id="how-it-works">How it works</h2>
<p>Periscope monitors the application progress, the number of YARN containers/resources and their allocation, queue depths, and the number of available cluster nodes and their health.
Since we have switched to YARN a while ago (been among the first adopters) we have run an open source <a href="https://github.com/sequenceiq/yarn-monitoring">monitoring project</a>, based on R.
We have been collecting metrics from the YARN Timeline server, Hadoop Metrics2 and Ambari's Nagios/Ganglia - and profiling the applications and correlating with these metrics.
One of the key findings was that while low level metrics are good to understand the cluster health - they might not necessarily help on making decisions when applying different SLA policies on a multi-tenant cluster.
Focusing on higher level building blocks as queue depth, YARN containers, etc actually brings in the same quality of service, while not being lost in low level details.</p>
<p>Periscope works with two types of Hadoop clusters: <code>static</code> and <code>dynamic</code>. Periscope does not require any pre-installation - the only thing it requires is to be <code>attached</code> to an Ambari server's REST API.</p>
<h2 id="technology">Technology</h2>
<h3 id="cloudbreak">Cloudbreak</h3>
<p>Cloudbreak is SequenceIQ's RESTful Hadoop as a Service API. Once it is deployed in your favorite servlet container exposes a REST API allowing to span up Hadoop clusters of arbitrary sizes on your selected cloud provider. Provisioning Hadoop has never been easier. Cloudbreak is built on the foundation of cloud providers API (Amazon AWS, Microsoft Azure, Google Cloud Compute...), Apache Ambari, Docker containers, Serf and dnsmasq.</p>
<p>For further information please check the <a href="http://sequenceiq.com/cloudbreak">Cloudbreak documentation</a>.</p>
<h3 id="apache-yarn">Apache YARN</h3>
<p>Since the emergence of Hadoop 2 and the YARN based architecture we have a platform where we can run multiple applications (of different types) not constrained only to MapReduce.</p>
<p>The idea of YARN is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). The ResourceManager and per-node slave, the NodeManager (NM), form the data-computation framework. The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. The per-application ApplicationMaster is, in effect, a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/periscope/master/docs/images/yarn-architecture.png" /></p>
<p>Different applications or different MapReduce job profiles have different resource needs, however since Hadoop 2 is a multi tenant platform the different users could have different access patterns or need for cluster capacity. In Hadoop 2.0 this is achieved through YARN schedulers â€” to allocate resources to various applications subject to constraints of capacities and queues. The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. The Scheduler is pure scheduler in the sense that it performs no monitoring or tracking of status for the application.</p>
<p>Periscope is using the Capacity Scheduler to apply SLA policies to applications.</p>
<h3 id="apache-ambari">Apache Ambari</h3>
<p>The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/periscope/master/docs/images/ambari-overview.png" /></p>
<p>Ambari enables System Administrators to:</p>
<ol>
<li>Provision a Hadoop Cluster</li>
<li>Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.</li>
<li>
<p>Ambari handles configuration of Hadoop services for the cluster.</p>
</li>
<li>
<p>Manage a Hadoop Cluster</p>
</li>
<li>
<p>Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire cluster.</p>
</li>
<li>
<p>Monitor a Hadoop Cluster</p>
</li>
<li>Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.</li>
<li>Ambari leverages Ganglia for metrics collection.</li>
<li>Ambari leverages Nagios for system alerting and will send emails when your attention is needed (e.g. a node goes down, remaining disk space is low, etc).</li>
</ol>
<p>Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.
Ambari Blueprints are a declarative definition of a cluster. With a Blueprint, you can specify a Stack, the Component layout and the Configurations to materialize a Hadoop cluster instance (via a REST API) without having to use the Ambari Cluster Install Wizard.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/periscope/master/docs/images/ambari-create-cluster.png" /></p>
<h3 id="docker">Docker</h3>
<p>Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications. Consisting of Docker Engine, a portable, lightweight runtime and packaging tool, and Docker Hub, a cloud service for sharing applications and automating workflows, Docker enables apps to be quickly assembled from components and eliminates the friction between development, QA, and production environments. As a result, IT can ship faster and run the same app, unchanged, on laptops, data center VMs, and any cloud.</p>
<p>The main features of Docker are:</p>
<ol>
<li>Lightweight, portable</li>
<li>Build once, run anywhere</li>
<li>VM - without the overhead of a VM</li>
<li>Each virtualised application includes not only the application and the necessary binaries and libraries, but also an entire guest operating system</li>
<li>
<p>The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process in userspace on the host operating system, sharing the kernel with other containers.
    <img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/vm.png" /></p>
</li>
<li>
<p>Containers are isolated</p>
</li>
<li>It can be automated and scripted</li>
</ol>
<h3 id="consul">Consul</h3>
<p>Consul it is a tool for discovering and configuring services in your infrastructure. It provides several key features</p>
<ul>
<li>
<p>Service Discovery: Clients of Consul can provide a service, such as api or mysql, and other clients can use Consul to discover providers of a given service. Using either DNS or HTTP, applications can easily find the services they depend upon.</p>
</li>
<li>
<p>Health Checking: Consul clients can provide any number of health checks, either associated with a given service ("is the webserver returning 200 OK"), or with the local node ("is memory utilization below 90%"). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts.</p>
</li>
<li>
<p>Key/Value Store: Applications can make use of Consul's hierarchical key/value store for any number of purposes, including dynamic configuration, feature flagging, coordination, leader election, and more. The simple HTTP API makes it easy to use.</p>
</li>
<li>
<p>Multi Datacenter: Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/consul.png" /></p>
</li>
</ul>
<h3 id="serf">Serf</h3>
<p>Serf is a tool for cluster membership, failure detection, and orchestration that is decentralised, fault-tolerant and highly available. Serf runs on every major platform: Linux, Mac OS X, and Windows. It is extremely lightweight.
Serf uses an efficient gossip protocol to solve three major problems:</p>
<ul>
<li>
<p>Membership: Serf maintains cluster membership lists and is able to execute custom handler scripts when that membership changes. For example, Serf can maintain the list of Hadoop servers of a cluster and notify the members when nodes come online or go offline.</p>
</li>
<li>
<p>Failure detection and recovery: Serf automatically detects failed nodes within seconds, notifies the rest of the cluster, and executes handler scripts allowing you to handle these events. Serf will attempt to recover failed nodes by reconnecting to them periodically.
    <img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-gossip.png" /></p>
</li>
<li>
<p>Custom event propagation: Serf can broadcast custom events and queries to the cluster. These can be used to trigger deploys, propagate configuration, etc. Events are simple fire-and-forget broadcast, and Serf makes a best effort to deliver messages in the face of offline nodes or network partitions. Queries provide a simple realtime request/response mechanism.
    <img alt="" src="https://raw.githubusercontent.com/sequenceiq/cloudbreak/master/docs/images/serf-event.png" /></p>
</li>
</ul>
<h2 id="building-blocks">Building blocks</h2>
<h3 id="clusters">Clusters</h3>
<p>A Hadoop cluster is a set of components and services launched in order to store, analyze and process unstructured data. Periscope can work with any Hadoop 2/ YARN cluster provisioned with Apache Ambari, and supports any YARN application.
As highlighted before, Periscope can apply SLA policies to <code>static</code> and <code>autoscaling</code> clusters. Due to flexibility supported by cloud based Hadoop deployments, we suggest to link Periscope with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> and apply policy based <code>autoscaling</code> to your cluster.</p>
<p><strong>Static clusters</strong>
From Periscope point of view we consider a cluster <code>static</code> when the cluster capacity can't be increased horizontally.
This means that the hardware resources are already given - and the throughput can't be increased by adding new nodes.
Periscope introspects the job submission process, monitors the applications and applies the following SLAs:</p>
<ol>
<li>Application ordering - can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li>Moves running applications between priority queues</li>
<li><em>Attempts</em> to enforce time based SLA (execution time, finish by, finish between, recurring)</li>
<li><em>Attempts</em> to enforce guaranteed cluster capacity requests ( x % of the resources)</li>
<li>Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li>Attach priorities to SLAs</li>
</ol>
<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>
<p><strong>Autoscaling clusters</strong>
From Periscope point of view we consider a cluster <code>dynamic</code> when the cluster capacity can be increased horizontally.
This means that nodes can be added or removed on the fly - thus the clusterâ€™s throughput can be increased or decreased based on the cluster load and scheduled applications.
Periscope works with <a href="http://sequenceiq.com/cloudbreak/">Cloudbreak</a> to add or remove nodes from the cluster based on the SLA policies and thus continuously provide a high <em>quality of service</em> for the multi-tenand Hadoop cluster.
Just to refresh memories - <a href="http://sequenceiq.com/products.html">Cloudbreak</a> is <a href="http://sequenceiq.com">SequenceIQ's</a> open source, cloud agnostic Hadoop as a Service API.
Given the option of provisioning or decommissioning cluster nodes on the fly, Periscope allows you to use the following set of SLAs:</p>
<ol>
<li>Application ordering - can guarantee that a higher priority application finishes before another one (supporting parallel or sequential execution)</li>
<li>Moves running applications between priority queues</li>
<li><em>Enforce</em> time based SLA (execution time, finish by, finish between, recurring) by increasing cluster capacity and throughput</li>
<li>Smart decommissioning - avoids HDFS storms, keeps <code>paid</code> nodes alive till the last minute</li>
<li><em>Enforce</em> guaranteed cluster capacity requests ( x % of the resources)</li>
<li><em>Private</em> cluster requests - supports provisioning of short lived private clusters with the possibility to merge</li>
<li>Support for distributed (but not YARN ready) applications using Apache Slider</li>
<li>Attach priorities to SLAs</li>
</ol>
<p><em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, YARN and Ambari and they will be included in the next releases (2.6 and 1.7)</em></p>
<h3 id="alarms">Alarms</h3>
<p>An alarm watches a <code>metric</code> over a specified time period, and used by one or more action or scaling policy based on the value of the metric relative to a given threshold over a number of time periods. In case Periscope raises an alarm an action (e.g. sending an email) or a scaling policy is triggered. Alarms are based on metrics. The current supported <code>metrics</code> are:
*<code>PENDING_CONTAINERS</code>- pending YARN containers</p>
<p>*<code>PENDING_APPLICATIONS</code> - pending/queued YARN applications</p>
<p>*<code>LOST_NODES</code> - cluster nodes lost</p>
<p>*<code>UNHEALTHY_NODES</code> - unhealthy cluster nodes</p>
<p>*<code>GLOBAL_RESOURCES</code> - global resources</p>
<p>Measured <code>metrics</code> are compared with pre-configured values using operators. The <code>comparison operators</code> are: <code>LESS_THAN</code>, <code>GREATER_THAN</code>, <code>LESS_OR_EQUAL_THAN</code>, <code>GREATER_OR_EQUAL_THAN</code>, <code>EQUALS</code>.
In order to avoid reacting for sudden spikes in the system and apply policies only in case of a sustained system stress, <code>alarms</code> have to be sustained over a <code>period</code> of time.  The <code>period</code> specifies the time period in minutes during the alarm has to be sustained.</p>
<p>Also a <code>threshold</code> can be configured, which specifies the variance applied by the operator for the selected <code>metric</code>.</p>
<h3 id="sla-scaling-policies">SLA Scaling Policies</h3>
<p>Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application.
When scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.
Periscope will do the heavy lifting and based on the alarms and the scaling policy linked to them it executes the associated policy.
By default a fully configured and running <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> cluster contains no SLA policies.
An SLA scaling policy can contain multiple alarms. As an alarm is triggered a  <code>scalingAdjustment</code> is applied, however to keep the cluster size within boundaries a <code>minSize</code> and <code>maxSize</code> is attached to the cluster - thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a <code>cooldown</code> period (minutes) - though an alarm is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered rules are applied in order.</p>
<h3 id="applications">Applications</h3>
<p>A Hadoop YARN application is a packaged workload submitted to a cluster. An application requests resources from YARN Resource Manager. The resources are allocated as YARN containers. By default Periscope works with the Hadoop YARN Capacity Scheduler. Using the Capacity Scheduler applications are submitted in different priority queues. The queue configurations, their depth, associated resources, etc have to be designed ahead - and adapted in case of new tenants, applications or workloads are using the cluster.
At SequenceIQ, through our contributions to Apache YARN we facilitate moving applications between queues - and thus use the SLA policies attached to these queues. Even more, those SLA policies which were previously attached to Capacity Scheduler queues now can be attached to submitted jobs/applications.
Also we facilitate changing the resources allocated to a running application - even though they were submitted and already running.
<em>Note: not all of the features above are supported in the first <code>public beta</code> version. There are dependencies we contributed to Hadoop, Ambari and YARN and they will be included in the next releases (1.7 and 2.6)</em></p>
<h3 id="configuration">Configuration</h3>
<p>Periscope brings in the capability to reconfigure a running cluster - in particular resource properties heavily used. These properties currently are mostly related to the Capacity Scheduler configurations, but as we add functionality to Periscope this set of properties will constantly increase.</p>
<h2 id="quickstart-and-installation">QuickStart and installation</h2>
<p>Periscope requires an Apache Ambari endpoint of your Hadoop cluster to start to apply your SLA policies. We suggest to start with <a href="http://sequenceiq.com/cloudbreak/#quickstart-and-installation">Cloudbreak</a>. Create a hosted free <a href="https://cloudbreak.sequenceiq.com/">Cloudbreak</a> account and start experimenting.</p>
<h3 id="build-and-run">Build and run</h3>
<p>The Periscope code is available at our <a href="https://github.com/sequenceiq/periscope.git">GitHub repository</a>.</p>
<pre class="prettyprint well"><code>git clone https://github.com/sequenceiq/periscope.git
cd periscope
</code></pre>

<p>In order to build Periscope you will need <a href="http://www.gradle.org/">Gradle</a>.</p>
<pre class="prettyprint well"><code>./gradlew clean build
</code></pre>

<p>You are almost done. In order to start using Periscope you will need to set or pass the following environment variables.</p>
<pre class="prettyprint well"><code>periscope.cloudbreak.url - URL of Cloudbreak, e.g: http://cloudbreak.sequenceiq.com:80
periscope.db.tcp.addr - Address of the Database, e.g: 172.17.0.2
periscope.db.tcp.port - Port of the Database, e.g: 5432
periscope.db.user - Username to the Database, default: postgres
periscope.db.pass - Password to the Database, default: &lt;no_password&gt;
periscope.db.name - Name of the Database, default: postgres
periscope.db.hbm2ddl.strategy - Strategy whether to create or update the DB scheme on start, default: update
periscope.smtp.host - SMTP host for sending emails
periscope.smtp.port - SMTP port
periscope.smtp.username - SMTP username
periscope.smtp.password - SMTP password
periscope.smtp.from - SMTP from address, e.g. no-reply@somedomain.com
periscope.identity.server.url - URL of the UAA identity server, e.g: http://uaa.sequenceiq.com:80
periscope.client.id - ID of the registered application in UAA
periscope.client.secret - Secret key of the registered application in UAA
</code></pre>

<p>Monitoring requests and actions are async which means they run in a different thread under a thread pool. You can configure
this pool, or you can leave the default values. These properties are optional.</p>
<pre class="prettyprint well"><code>periscope.threadpool.core.size - default: 10 - Base pool size
periscope.threadpool.max.size - default: 100 - Maximum number of parallel requests
periscope.threadpool.queue.size - default: 10 - Requests queue size
</code></pre>

<p>Periscope is a <a href="http://projects.spring.io/spring-boot/">Spring Boot</a> based application. In order to start please run the following.</p>
<pre class="prettyprint well"><code>java -jar periscope.jar
</code></pre>

<h2 id="monitoring">Monitoring</h2>
<p>Part of Periscope we have a Hadoop cluster monitoring solution called <a href="http://blog.sequenceiq.com/blog/2014/10/07/hadoop-monitoring/">Baywatch</a>.</p>
<p>Although various solutions have been created in the software industry for monitoring of activities taking place in a cluster, but it turned out that only a very few of them satisfies most of our needs. When we made the decision about which monitoring libraries and components to integrate in our stack we kept in mind that it needs to be:</p>
<ul>
<li>
<p><strong>scalable</strong> to be able to efficiently monitor small Hadoop clusters which are consisting of only a few nodes and also clusters which containing thousands of nodes</p>
</li>
<li>
<p><strong>flexible</strong> to be able to provide overview about the health of the whole cluster or about the health of individual nodes or even dive deeper into the internals of Hadoop, e.g. shall be able to visualize how our autoscaling solution for Hadoop YARN called  <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> moves running applications between <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a></p>
</li>
<li>
<p><strong>extensible</strong> to be able to use the gathered and stored data by extensions written by 3rd parties, e.g. a module which processes the stored (metrics) data and does real-time anomaly detection</p>
</li>
<li>
<p><strong>zero-configuration</strong> to be able to plug into any existing Hadoop cluster without additional configuration, component installation</p>
</li>
</ul>
<p>Based on the requirements above our choice were the followings:</p>
<ul>
<li><a href="http://logstash.net">Logstash</a> for log/metrics enrichment, parsing and transformation</li>
<li><a href="http://www.elasticsearch.org">Elasticsearch</a> for data storage, indexing</li>
<li><a href="http://www.elasticsearch.org/overview/kibana">Kibana</a> for data visualization</li>
</ul>
<h3 id="high-level-architecture">High Level Architecture</h3>
<p>In our monitoring solution one of the design goal was to provide a <strong>generic, pluggable and isolated monitoring component</strong> to existing Hadoop deployments. We also wanted to make it non-invasive and avoid adding any monitoring related dependency to our Ambari, Hadoop or other Docker images. For that reason we have packaged the monitoring client component into its own Docker image which can be launched alongside with a Hadoop running in another container or even alongside a Hadoop which is not even containerized.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop-monitoring-arch.png" /></p>
<p>In a nutshell the monitoring solution consist of client and server containers. The <code>server</code> contains the Elasticsearch and the Kibana module. The server container is horizontally scalable and it can be clustered trough the clustering capabilities of Elasticsearch.</p>
<p>The <code>client</code> container - which is deployed on the machine what is needed to be monitored - contains the Logstash and the collectd module. The Logstash connects to Elasticsearch cluster as client and stores the processed and transformed metrics data there.</p>
<h3 id="hadoop-metrics">Hadoop metrics</h3>
<p>The metrics data what we are collecting and visualizing are provided by <a href="http://blog.cloudera.com/blog/2012/10/what-is-hadoop-metrics2">Hadoop metrics</a>, which is a collection of runtime information that are exposed by all Hadoop daemons. We have configured the Metrics subsystem in that way that it writes the valuable metrics information into the filesystem.</p>
<p>In order to be able to access the metrics data from the monitoring client component - which is running inside a different Docker container - we used the capability of <a href="https://docs.docker.com/userguide/dockervolumes">Docker Volumes</a> which basically let's you access a directory within one container form other container or even access directories from host systems.</p>
<p>For example if you would like mount the <code>/var/log</code> from the container named <code>ambari-singlenode</code> under the <code>/amb/log</code> in the monitoring client container then the following sequence of commands needs to be executed:</p>
<pre class="prettyprint well"><code class="bash">EXPOSED_LOG_DIR=$(docker inspect --format='{{index .Volumes &quot;/var/log&quot;}}' ambari-singlenode)
docker run -i -t -v $EXPOSED_LOG_DIR:/amb/log  sequenceiq/baywatch-client /etc/bootstrap.sh -bash
</code></pre>

<p>Hundreds of different metrics are gathered form Hadoop metrics subsystem and all data is transformed by Logstash to JSON and stored in ElasticSearch to make it ready for querying or displaying it with Kibana.</p>
<p>The screenshot below has been created from one of our sample dashboard which is displaying Hadoop metrics for a small cluster which was started on my notebook. In this cluster the Yarn's Capacity Scheduler is used and for demonstration purposes I have created a queue called <code>highprio</code> alongside the <code>default</code> queue. I have reduced the capacity of the <code>default</code> queue to 30 and defined the <code>highprio</code> queue with a capacity of 70.
The red line in the screenshot belongs to the <code>highprio</code> queue, the yellow line belongs to the <code>default</code> queue and the green line is the <code>root</code> queue which is the common ancestor both of them.
In the benchmark, the jobs were submitted to the <code>default</code> queue and a bit later (somewhere around 17:48) the same jobs were submitted to the <code>highprio</code> queue. As it is clearly observable for <code>highprio</code> queue the allocated Containers, Memory and VCores were higher and jobs were finished much more faster than those that were submitted to the default queue.</p>
<p>Such kind of dashboard is extremely useful when we are visualizing decisions made by <a href="http://blog.sequenceiq.com/blog/2014/08/27/announcing-periscope">Periscope</a> and check e.g. how the applications are moved across <a href="http://blog.sequenceiq.com/blog/2014/07/02/move-applications-between-queues">queues</a>, or additional nodes are added or removed dynamically from the cluster.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png" /></p>
<p>To see it in large, please <a href="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/hadoop_metrics.png">click here</a>.</p>
<p>Since all of the Hadoop metrics are stored in the Elasticsearch, therefore there are a lot of possibilities to create different dashboards using that particular parameter of the cluster which is interesting for the operator. The dashboards can be configured on the fly and the metrics are displayed in real-time.</p>
<h3 id="system-resources">System resources</h3>
<p>Beside Hadoop metrics, "traditional" system resource data (cpu, memory, io, network) are gathered with the aid of <a href="https://collectd.org">collectd</a>. This can also run inside the monitoring client container since due to the <a href="https://goldmann.pl/blog/2014/09/11/resource-management-in-docker/#_example_managing_the_cpu_shares_of_a_container">resource management</a> in Docker the containers can access and gather information about the whole system and a container can even "steal" the network of other container if you start with: <code>--net=container:id-of-other-container</code> which is very useful if cases when network traffic is monitored.</p>
<p><img alt="" src="https://raw.githubusercontent.com/sequenceiq/sequenceiq-samples/master/hadoop-monitoring/system_resource_metrics.png" /></p>
<h2 id="releases-future-plans">Releases, future plans</h2>
<p>Quite a while ago we have been thinking about <code>autoscaling</code> Hadoop clusters. First of all - being a cost aware young startup - we always had to manually manage our cloud based VM instances, doing what exactly Periscope does. Having short and long running Hadoop jobs on different clusters, and maintaining in parallel different clusters it was a very error prone and tedious job. While Amazon for instance gives a quite good API (remember we always use a CLI as an alternative for UI) but this still wasnâ€™t easy when you have 10+ clusters of different sizes. On the other hand we have started to use different cloud providers as well - Microsoftâ€™s Azure and Googleâ€™s Cloud Compute.
This diversity started to eat into too much DevOps time - and we decided to automate everything and create Periscope.</p>
<h3 id="public-beta-01">Public Beta - 0.1</h3>
<p>The <code>first public beta</code> does support autoscaling clusters on <strong>Amazon AWS</strong> and <strong>Microsoft Azure</strong> - and we will bring in the other Cloudbreak providers as we add them. Once our contributions in Apache Hadoop, YARN and Ambari will be released (patches are accepted and in trunk - target versions are <em>2.6.0 and 1.7.0</em> - Periscope will start supporting the <code>static</code> cluster features such as application SLA policies.
Also note that the current version supports only up-scaling.
The currently supported Hadoop is the Hortonworks Data Platform - the 100% open source Hadoop distribution and the respective component versions are:</p>
<p>CentOS - 6.5 Hortonworks Data Platform - 2.1 Apache Hadoop - 2.4.0 Apache Tez - 0.4 Apache Pig - 0.12.1 Apache Hive &amp; HCatalog - 0.13.0 Apache HBase - 0.98.0 Apache Phoenix - 4.0.0 Apache Accumulo - 1.5.1 Apache Storm - 0.9.1 Apache Mahout - 0.9.0 Apache Solr - 4.7.2 Apache Falcon - 0.5.0 Apache Sqoop - 1.4.4 Apache Flume - 1.4.0 Apache Ambari - 1.6.1 Apache Oozie - 4.0.0 Apache Zookeeper - 3.4.5 Apache Knox - 0.4.0 Docker - 1.1 Serf - 0.5.0 dnsmasq - 2.7</p>
<h3 id="future-releases">Future releases</h3>
<p>While this is already a good achievement - bringing autoscaling to cloud based Hadoop clusters - we donâ€™t stop here. Periscope will be a centralized place to manage your cluster through SLA policies, check your cluster metrics and logs and correlate them with events/cluster heath - built on the well proven ELK stack (Elasticsearch, Logstash, Kibana). The analytics and visualization capabilities will allow a deeper understanding of running jobs, the nature of resources consumed and ultimately leverage the features provided by cloud providers. For instance a CPU heavy job can always launch purpose built (compute optimized) instance types.
The next release will bring in OAuth2 support - and will incorporate the new features from YARN, Hadoop and Ambari, thus you will be able to attach SLA policies to static clusters. As we have already mentioned we are running a YARN monitoring project based on R - based on the experience and what we have learnt the end goal is to built a high level heuristic model which maintains a healthy cluster, without the need of predefined SLA policy rules.</p>
<h2 id="contribution">Contribution</h2>
<p>So you are about to contribute to Periscope? Awesome! There are many different ways in which you can contribute. We strongly value your feedback, questions, bug reports, and feature requests.
Periscope consist of the following main projects:</p>
<h3 id="periscope-code">Periscope code</h3>
<p>Available: <a href=https://github.com/sequenceiq/periscope>https://github.com/sequenceiq/periscope</a></p>
<h3 id="periscope-api">Periscope API</h3>
<p>Available: <a href=https://periscope-api.sequenceiq.com>https://periscope-api.sequenceiq.com</a></p>
<p>GitHub: <a href=https://github.com/sequenceiq/periscope>https://github.com/sequenceiq/periscope</a></p>
<h3 id="periscope-documentation">Periscope documentation</h3>
<p>Product documentation: <a href=http://sequenceiq.com/periscope>http://sequenceiq.com/periscope</a></p>
<p>GitHub: <a href=https://github.com/sequenceiq/periscope/blob/master/docs/index.md>https://github.com/sequenceiq/periscope/blob/master/docs/index.md</a></p>
<p>API documentation: <a href=http://docs.periscope.apiary.io>http://docs.periscope.apiary.io</a></p>
<p>GitHub: <a href=https://github.com/sequenceiq/periscope/blob/master/apiary.apib>https://github.com/sequenceiq/periscope/blob/master/apiary.apib</a></p>
<h3 id="ways-to-contribute">Ways to contribute</h3>
<ul>
<li>Use Periscope and Cloudbreak</li>
<li>Submit a GitHub issue to the appropriate GitHub repository.</li>
<li>Submit a new feature request (as a GitHub issue).</li>
<li>Submit a code fix for a bug.</li>
<li>Submit a unit test.</li>
<li>Code review pending pull requests and bug fixes.</li>
<li>Tell others about these projects.</li>
</ul>
<h3 id="contributing-code">Contributing code</h3>
<p>We are always thrilled to receive pull requests, and do our best to process them as fast as possible. Not sure if that typo is worth a pull request? Do it! We will appreciate it.
The Periscope projects are open source and developed/distributed under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache Software License, Version 2.0</a>.
If you wish to contribute to Periscope (which you're very welcome and encouraged to do so) then you must agree to release the rights of your source under this license.</p>
<h4 id="creating-issues">Creating issues</h4>
<p>Any significant improvement should be documented as a GitHub issue before starting to work on it. Please use the appropriate labels - bug, enhancement, etc - this helps while creating the release notes for a version release.
Before submitting issues please check for duplicate or similar issues. If you are unclear about an issue please feel free to <a href="https://groups.google.com/forum/#!forum/sequenceiq-periscope">contact us</a>.</p>
<h4 id="discuss-your-design">Discuss your design</h4>
<p>We recommend discussing your plans on the <a href="https://groups.google.com/forum/#!forum/periscope">mailing list</a> before starting to code - especially for more ambitious contributions. This gives other contributors a chance to point you in the right direction, give feedback on your design, and maybe point out if someone else is working on the same thing.</p>
<h4 id="conventions">Conventions</h4>
<p>Please write clean code. Universally formatted code promotes ease of writing, reading, and maintenance.
* Do not use @author tags.</p>
<ul>
<li>
<p>New classes must match our dependency mechanism.</p>
</li>
<li>
<p>Code must be formatted according to our <a href="https://github.com/sequenceiq/periscope/blob/master/config/eclipse_formatter.xml">formatter</a>.</p>
</li>
<li>
<p>Code must be checked with our <a href="https://github.com/sequenceiq/periscope/tree/master/config/checkstyle">checkstyle</a>.</p>
</li>
<li>
<p>Contributions must pass existing unit tests.</p>
</li>
<li>
<p>The code changes must be accompanied by unit tests. In cases where unit tests are not possible or donâ€™t make sense an explanation should be provided.</p>
</li>
<li>
<p>New unit tests should be provided to demonstrate bugs and fixes (use Mockito whenever possible).</p>
</li>
<li>
<p>The tests should be named *Test.java.</p>
</li>
<li>
<p>Use slf4j instead of commons logging as the logging facade.</p>
</li>
</ul>
<h3 id="thank-you">Thank you</h3>
<p>Huge thanks go to the contributors from the community who have been actively working with the SequenceIQ team. Kudos for that.</p>
<h3 id="legal">Legal</h3>
<p><em>Brought to you courtesy of our legal counsel.</em></p>
<p>Use and transfer of Periscope may be subject to certain restrictions by the
United States and other governments.<br />
It is your responsibility to ensure that your use and/or transfer does not
violate applicable laws.</p>
<h3 id="licensing">Licensing</h3>
<p>Periscope is licensed under the Apache License, Version 2.0. See <a href="http://www.apache.org/licenses/LICENSE-2.0.html">LICENSE</a> for full license text.</p>
</div>
        </div>

        

        <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
        <script src="./js/bootstrap-3.0.3.min.js"></script>
        <script src="./js/prettify-1.0.min.js"></script>
        <script src="./js/base.js"></script>
    </body>
</html>